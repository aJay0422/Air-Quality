{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d5233\\anaconda3\\envs\\env_ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "from haversine import haversine\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from time import time\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "project_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from utils.data import load_data\n",
    "from utils.tool import prediction_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File1: 3BAGEmnnQ2K4zF49Dkkoxg.csv contains missing hours\n",
      "File4: 4XEJFVFOS761cvyEjOYf0g.csv contains outliers\n",
      "File5: 6kzhfU9xTKCUVJMz492l2g.csv contains outliers\n",
      "File6: 6nBLCf6WT06TOuUExPkBtA.csv contains missing hours\n",
      "File17: JQ1px-xqQx-xKh3Oa5h9nA.csv contains missing hours\n",
      "File21: OfAvTbS1SiOjQo4WKSAP9g.csv contains missing hours\n",
      "File24: R2ebpAblQHylOjteA-2hlQ.csv contains missing hours\n",
      "File37: jDYxIP2JQL2br5aTIAR7JQ.csv contains outliers\n",
      "File38: kyRUtBOTTaK7V_-dxOJTwg.csv contains outliers\n",
      "File45: wSo2iRgjT36eWC4a2joWZg.csv contains outliers\n"
     ]
    }
   ],
   "source": [
    "os.chdir(project_dir)\n",
    "train_loader, val_loader, test_loader = load_data(window=2, batch_size=32)\n",
    "os.chdir(os.path.join(project_dir, 'models/GNN10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGraphDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        super(MyGraphDataset, self).__init__()\n",
    "        self.graphs = graphs\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.graphs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_edge_index(n):\n",
    "    \"\"\"\n",
    "    Create edge index for a complete graph of n nodes with self loops\n",
    "    :param n: number of nodes\n",
    "    :return: edge index tensor\n",
    "    \"\"\"\n",
    "    edge_index = torch.tensor([[i, j] for i in range(n) for j in range(n)], dtype=torch.long).t().contiguous()\n",
    "    return edge_index\n",
    "\n",
    "def get_distance_matrix(locs):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance matrix\n",
    "    :param locs: a torch tensor of size (batch_size, num_nodes, 2) or (num_nodes, 2)\n",
    "    \"\"\"\n",
    "    if len(locs.size()) == 2:\n",
    "        batch = False\n",
    "        locs = locs.unsqueeze(0)\n",
    "    else:\n",
    "        batch = True\n",
    "\n",
    "\n",
    "    B, N, _ = locs.size()\n",
    "    locs_i = locs.unsqueeze(2).expand(B, N, N, 2)\n",
    "    locs_j = locs.unsqueeze(1).expand(B, N, N, 2)\n",
    "    distances = torch.sqrt(((locs_i - locs_j) ** 2).sum(dim=-1))\n",
    "\n",
    "    if not batch:\n",
    "        distances = distances.squeeze(0)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Following 2 functions adapted from https://github.com/nnzhan/Graph-WaveNet/blob/master/util.py\n",
    "def calculate_scaled_laplacian(adj_mx, lambda_max=2, undirected=False):\n",
    "    def calculate_normalized_laplacian(adj):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        d = np.array(adj.sum(1))\n",
    "        d_inv_sqrt = np.power(d, -0.5).flatten()\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "        normalized_laplacian = sp.eye(adj.shape[0]) - adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "        return normalized_laplacian\n",
    "    if undirected:\n",
    "        adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
    "    L = calculate_normalized_laplacian(adj_mx)\n",
    "    if lambda_max is None:\n",
    "        lambda_max, _ = linalg.eigsh(L, 1, which='LM')\n",
    "        lambda_max = lambda_max[0]\n",
    "    L = sp.csr_matrix(L)\n",
    "    M, _ = L.shape\n",
    "    I = sp.identity(M, format='csr', dtype=L.dtype)\n",
    "    L = (2 / lambda_max * L) - I\n",
    "    return L.astype(np.float32).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 2 * out_dim),\n",
    "            nn.BatchNorm1d(2 * out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * out_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "\n",
    "class LocalLayer(MessagePassing):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(LocalLayer, self).__init__(aggr='mean')\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = Mlp(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        :param x: node features of shape (num_nodes, in_dim)\n",
    "        :param edge_index: edge index tensor of shape (2, num_edges)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        # transform node features\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # compute normalization\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, N, dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # propagate message\n",
    "        outputs = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "    \n",
    "\n",
    "class DiffusionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(DiffusionLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.mlp = Mlp(in_dim, out_dim)\n",
    "\n",
    "        self.k = Parameter(torch.Tensor(1, out_dim))\n",
    "\n",
    "    def forward(self, locs, features):\n",
    "        \"\"\"\n",
    "        :param locs: (B, N, 2)\n",
    "        :param features: (B, N, in_dim)\n",
    "        \"\"\"\n",
    "        B, N, D = features.size()\n",
    "\n",
    "        # apply mlp to features\n",
    "        features = rearrange(features, 'b n d -> (b n) d')\n",
    "        features = self.mlp(features)\n",
    "        features = rearrange(features, '(b n) d -> b n d', b=B)\n",
    "\n",
    "        # calculate the scaled laplacian\n",
    "        L = np.zeros((B, N, N))\n",
    "        for b in range(B):\n",
    "            D = get_distance_matrix(locs[b])\n",
    "            W = 1 / (D + 1e-8)\n",
    "            # Set diagonal elements to 0\n",
    "            mask = torch.ones(N, N).to(features.device) - torch.eye(N).to(features.device)\n",
    "            W = W * mask\n",
    "            L_temp = calculate_scaled_laplacian(W.cpu().numpy())\n",
    "            L[b] = L_temp\n",
    "        # apply diffusion\n",
    "        L = torch.from_numpy(L).to(features.device).float()   # (B, N, N)\n",
    "        outputs = torch.bmm(L, features) * self.k\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "class ConvectionLayer(MessagePassing):\n",
    "    def __init__(self, in_dim, out_dim, edge_dim):\n",
    "        super(ConvectionLayer, self).__init__(aggr='mean')\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.edge_in_dim = edge_dim\n",
    "\n",
    "        self.node_linear = Mlp(in_dim, out_dim)\n",
    "        self.edge_linear = Mlp(edge_dim, out_dim)\n",
    "        self.message_linear = Mlp(2 * out_dim, out_dim)\n",
    "        self.update_linear = Mlp(2 * out_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        :param x: node features of shape (num_nodes, in_dim)\n",
    "        :param edge_index: edge index tensor of shape (2, num_edges)\n",
    "        :param edge_attr: edge features of shape (num_edges, edge_dim)\n",
    "        \"\"\"\n",
    "        assert len(x.size()) == len(edge_attr.size()) == 2, \"x and edge_attr must have 2 dimensions\"\n",
    "        assert edge_index.size(1) == edge_attr.size(0), \"edge_index and edge_attr must have the same number of edges\"\n",
    "\n",
    "        N = x.size(0)\n",
    "        # transform node and edge features\n",
    "        x = self.node_linear(x)\n",
    "        edge_attr = self.edge_linear(edge_attr)\n",
    "\n",
    "        outputs = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        return outputs, edge_attr\n",
    "    \n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        \"\"\"\n",
    "        Edge from j -> i\n",
    "        \"\"\"\n",
    "        x_i = x_i + edge_attr\n",
    "        x_j = x_j + edge_attr\n",
    "        x = torch.cat([x_i, x_j], dim=-1)\n",
    "        return self.message_linear(x) + x_i\n",
    "    \n",
    "    def update(self, aggr_out, x):\n",
    "        x_cat = torch.cat([aggr_out, x], dim=-1)\n",
    "        return self.update_linear(x_cat) + x\n",
    "    \n",
    "\n",
    "class LCDLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, edge_dim):\n",
    "        super(LCDLayer, self).__init__()\n",
    "\n",
    "        self.local = LocalLayer(in_dim, out_dim)\n",
    "        self.convection = ConvectionLayer(in_dim, out_dim, edge_dim)\n",
    "        self.diffusion = DiffusionLayer(in_dim, out_dim)\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(3 * out_dim, 3),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.weights = []\n",
    "\n",
    "    def forward(self, locs, features, winds_feature, edge_index):\n",
    "        \"\"\"\n",
    "        :param locs: a torch tensor of size (B, N, 2)\n",
    "        :param features: a torch tensor of size (B, N, in_dim)\n",
    "        :param winds_feature: a torch tensor of size (B, N^2, in_dim)\n",
    "        :param edge_index: a torch tensor of size (2, N^2)\n",
    "        \"\"\"\n",
    "        B, N, _ = locs.size()\n",
    "        # preprocess the input data into a batch\n",
    "        batch = self.batch_preprocess(locs, features, winds_feature, edge_index)\n",
    "\n",
    "        # input original feature to nn.Module and batch features to MessagePassing\n",
    "        l_features = self.local(batch.features, batch.edge_index)\n",
    "        c_features, edge_attr = self.convection(batch.features, batch.edge_index, batch.winds_feature)\n",
    "        d_features = self.diffusion(locs, features)\n",
    "\n",
    "        # reverse the batched outputs\n",
    "        l_features = self.reverse_batch_process(l_features, B=B)\n",
    "        c_features, edge_attr = self.reverse_batch_process(c_features, edge_attr, B=B)\n",
    "\n",
    "        # fusion\n",
    "        weights = self.fusion(torch.cat([l_features, c_features, d_features], dim=-1))   # (B, N, 3)\n",
    "        self.weights.append(weights[:, -1, :].detach().cpu())\n",
    "        outputs = l_features * weights[:, :, 0].unsqueeze(-1) + c_features * weights[:, :, 1].unsqueeze(-1) + d_features * weights[:, :, 2].unsqueeze(-1)\n",
    "\n",
    "        return outputs, edge_attr\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_preprocess(locs, features, winds_feature, edge_index):\n",
    "        \"\"\"\n",
    "        Preprocess the input data into a single batch\n",
    "        \"\"\"\n",
    "        B, N, _ = locs.size()\n",
    "        graphs = []\n",
    "        for b in range(B):\n",
    "            graph = Data(features=features[b],\n",
    "                         edge_index=edge_index,\n",
    "                         locs=locs[b],\n",
    "                         winds_feature=winds_feature[b],\n",
    "                         num_nodes=N)\n",
    "            graphs.append(graph)\n",
    "        graphs = MyGraphDataset(graphs)\n",
    "        loader = DataLoader(graphs, batch_size=B, shuffle=False)\n",
    "        return next(iter(loader))\n",
    "    \n",
    "    @staticmethod\n",
    "    def reverse_batch_process(features, edge_attr=None, B=32):\n",
    "        \"\"\"\n",
    "        Reverse the batch process\n",
    "        :param features: a torch tensor of size (B*N, out_dim)\n",
    "        :param edge_attr: a torch tensor of size (B*N**2, edge_dim)\n",
    "        \"\"\"\n",
    "        N = features.size(0) // B\n",
    "        features = rearrange(features, '(b n) d -> b n d', b=B)\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = rearrange(edge_attr, '(b m) d -> b m d', b=B)\n",
    "            return features, edge_attr\n",
    "        return features\n",
    "\n",
    "\n",
    "class LCDGCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim=1,\n",
    "                 hidden_dim=64,\n",
    "                 out_dim=1,\n",
    "                 num_layers=3,\n",
    "                 edge_dim=3):\n",
    "        super(LCDGCN, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(LCDLayer(in_dim, hidden_dim, edge_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(LCDLayer(hidden_dim, hidden_dim, hidden_dim))\n",
    "        self.layers.append(LCDLayer(hidden_dim, out_dim, hidden_dim))\n",
    "\n",
    "    def forward(self, locs, readings, target_loc, winds_feature, edge_index):\n",
    "        \"\"\"\n",
    "        :param locs: a torch tensor of size (B, N, 2)\n",
    "        :param readings: a torch tensor of size (B, N, in_dim, 2). Here 2 stands for 2 time steps\n",
    "        :param target_loc: a torch tensor of size (B, 2)\n",
    "        :param winds_feature: a torch tensor of size (B, N+1, N+1, edge_dim)\n",
    "        \"\"\"\n",
    "        B, N, D, T = readings.size()\n",
    "        assert N == winds_feature.size(1) - 1, f\"The number of nodes in winds_feature({winds_feature.size(1)}) should be 1 plus that in readings({N})\"\n",
    "\n",
    "        # preprocess edge features\n",
    "        winds_feature = self.preprocess_edge_features(winds_feature)\n",
    "\n",
    "        # concat and mask\n",
    "        lcd_locs = torch.cat([locs, target_loc[:, None, :]], dim=1)   # (B, N+1, 2)\n",
    "        lcd_readings = torch.cat([readings[:, :, :, 1],\n",
    "                                  torch.zeros(B, 1, D).to(readings.device)],\n",
    "                                  dim=1)   # (B, N+1, D)\n",
    "        \n",
    "        # forward pass\n",
    "        for layer in self.layers:\n",
    "            lcd_readings, winds_feature = layer(lcd_locs, lcd_readings, winds_feature, edge_index)\n",
    "        outputs = lcd_readings\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_edge_features(edge_features):\n",
    "        \"\"\"\n",
    "        Transform edge features from shape (B, N, N, D) to (B, N^2, D)\n",
    "        \"\"\"\n",
    "        assert len(edge_features.size()) == 4\n",
    "        assert edge_features.size(1) == edge_features.size(2)\n",
    "        B, N, _, D = edge_features.size()\n",
    "        edge_features = edge_features.view(B, N * N, D)\n",
    "        return edge_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "\n",
    "# set seed for model initialization\n",
    "seedi = 45\n",
    "torch.manual_seed(seedi)\n",
    "model = LCDGCN(in_dim=1,\n",
    "               hidden_dim=hidden_dim,\n",
    "               out_dim=1,\n",
    "               num_layers=num_layers,\n",
    "               edge_dim=3)\n",
    "\n",
    "# set up training\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:49<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 190.7743, Val Loss: 60.9291, Test Loss: 73.0414\n",
      "Save the best model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:58<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 32.3536, Val Loss: 70.1070, Test Loss: 80.2091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 23.4722, Val Loss: 61.7588, Test Loss: 72.3464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:03<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Train Loss: 21.0421, Val Loss: 66.6502, Test Loss: 62.3217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:02<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Train Loss: 19.8996, Val Loss: 33.3730, Test Loss: 38.4848\n",
      "Save the best model at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:59<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Train Loss: 19.4195, Val Loss: 44.7596, Test Loss: 47.4280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:59<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Train Loss: 18.7290, Val Loss: 37.3441, Test Loss: 39.7242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Train Loss: 18.3074, Val Loss: 44.0319, Test Loss: 41.8391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:02<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Train Loss: 18.0896, Val Loss: 36.7661, Test Loss: 40.0269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:02<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 17.4883, Val Loss: 34.6455, Test Loss: 35.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:59<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Train Loss: 17.4140, Val Loss: 36.2450, Test Loss: 36.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:01<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Train Loss: 16.9811, Val Loss: 34.7016, Test Loss: 40.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:02<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Train Loss: 16.8422, Val Loss: 44.9860, Test Loss: 40.7269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:59<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Train Loss: 16.3246, Val Loss: 56.2155, Test Loss: 50.2582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:57<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Train Loss: 16.2327, Val Loss: 39.3155, Test Loss: 33.9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:01<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 15.7913, Val Loss: 29.8270, Test Loss: 30.5824\n",
      "Save the best model at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:03<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Train Loss: 15.6058, Val Loss: 30.8259, Test Loss: 37.7521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:01<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Train Loss: 15.2288, Val Loss: 43.3505, Test Loss: 41.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:58<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Train Loss: 14.9987, Val Loss: 51.9523, Test Loss: 42.7103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: 14.7785, Val Loss: 48.0003, Test Loss: 36.8403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:03<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Train Loss: 14.6818, Val Loss: 35.7983, Test Loss: 35.2305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:01<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Train Loss: 14.4889, Val Loss: 42.4204, Test Loss: 33.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Train Loss: 14.4083, Val Loss: 35.7942, Test Loss: 29.7415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:02<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Train Loss: 14.1978, Val Loss: 35.1457, Test Loss: 36.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:02<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Train Loss: 14.1775, Val Loss: 45.8469, Test Loss: 39.1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Train Loss: 13.9814, Val Loss: 43.4737, Test Loss: 33.5158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:00<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Train Loss: 13.9939, Val Loss: 39.1391, Test Loss: 37.5066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:01<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Train Loss: 13.8643, Val Loss: 48.1678, Test Loss: 30.9791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:04<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Train Loss: 13.7387, Val Loss: 50.3166, Test Loss: 39.1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:03<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Train Loss: 13.6839, Val Loss: 35.2610, Test Loss: 35.6341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [08:59<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Train Loss: 13.3980, Val Loss: 40.1289, Test Loss: 34.4052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:30<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Train Loss: 13.4725, Val Loss: 54.1172, Test Loss: 37.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:22<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Train Loss: 13.4225, Val Loss: 47.3592, Test Loss: 40.5577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2562/2562 [09:01<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# set seed for training\n",
    "seedt = 45\n",
    "torch.manual_seed(seedt)\n",
    "\n",
    "# set weight path\n",
    "save_path = f\"./weights/LCDGCN_h{hidden_dim}_l{num_layers}_si{seedi}_st{seedt}.pth\"\n",
    "\n",
    "# training\n",
    "best_val = 1e10\n",
    "model = model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        locs, readings, target_loc, target_reading, winds_feature = batch\n",
    "        locs = locs.to(device)\n",
    "        readings = readings.to(device)\n",
    "        target_loc = target_loc.to(device)\n",
    "        target_reading = target_reading.to(device)\n",
    "        winds_feature = winds_feature.to(device)\n",
    "\n",
    "        readings_input = readings[..., None].permute(0, 2, 3, 1)   # (B, N, 1, 2)\n",
    "        winds_feature_input = winds_feature[:, :, :, 0, :]   # (B, N+1, N+1, 3)\n",
    "        pred_target = torch.cat([readings.permute(0, 2, 1)[:, :, 1:],\n",
    "                                 target_reading[:, 1:].unsqueeze(1)],\n",
    "                                 dim=1)   # (B, N+1, 1)\n",
    "        edge_index = get_edge_index(pred_target.size(1)).to(device)\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(locs, readings_input, target_loc, winds_feature_input, edge_index)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(pred[:, -1], pred_target[:, -1])\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            locs, readings, target_loc, target_reading, winds_feature = batch\n",
    "            locs = locs.to(device)\n",
    "            readings = readings.to(device)\n",
    "            target_loc = target_loc.to(device)\n",
    "            target_reading = target_reading.to(device)\n",
    "            winds_feature = winds_feature.to(device)\n",
    "\n",
    "            readings_input = readings[..., None].permute(0, 2, 3, 1)\n",
    "            winds_feature_input = winds_feature[:, :, :, 0, :]\n",
    "            pred_target = torch.cat([readings.permute(0, 2, 1)[:, :, 1:],\n",
    "                                     target_reading[:, 1:].unsqueeze(1)],\n",
    "                                     dim=1)\n",
    "            edge_index = get_edge_index(pred_target.size(1)).to(device)\n",
    "\n",
    "            # forward\n",
    "            pred = model(locs, readings_input, target_loc, winds_feature_input, edge_index)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(pred[:, -1], pred_target[:, -1])\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    # test\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in test_loader:\n",
    "            locs, readings, target_loc, target_reading, winds_feature = batch\n",
    "            locs = locs.to(device)\n",
    "            readings = readings.to(device)\n",
    "            target_loc = target_loc.to(device)\n",
    "            target_reading = target_reading.to(device)\n",
    "            winds_feature = winds_feature.to(device)\n",
    "\n",
    "            readings_input = readings[..., None].permute(0, 2, 3, 1)\n",
    "            winds_feature_input = winds_feature[:, :, :, 0, :]\n",
    "            pred_target = torch.cat([readings.permute(0, 2, 1)[:, :, 1:],\n",
    "                                     target_reading[:, 1:].unsqueeze(1)],\n",
    "                                     dim=1)\n",
    "            edge_index = get_edge_index(pred_target.size(1)).to(device)\n",
    "\n",
    "            # forward\n",
    "            pred = model(locs, readings_input, target_loc, winds_feature_input, edge_index)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(pred[:, -1], pred_target[:, -1])\n",
    "            test_losses.append(loss.item())\n",
    "        \n",
    "        # print training and validation losses\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {np.mean(losses):.4f}, Val Loss: {np.mean(val_losses):.4f}, Test Loss: {np.mean(test_losses):.4f}\")\n",
    "\n",
    "        # save the best model\n",
    "        if np.mean(val_losses) < best_val:\n",
    "            best_val = np.mean(val_losses)\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Save the best model at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
